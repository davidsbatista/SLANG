{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Paramopama + HAREM II datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "sentences = []\n",
    "tags = []\n",
    "max_len_sentence = 0\n",
    "\n",
    "tags_freq = defaultdict(int)\n",
    "tokens_freq = defaultdict(int)\n",
    "\n",
    "current_sentence_tokens = []\n",
    "current_sentence_tags = []\n",
    "\n",
    "with open(\"/Users/dsbatista/NER-datasets/Portuguese/Paramopama/corpus_paramopama+second_harem.txt\", 'rt') as f_in:\n",
    "    for line in f_in:\n",
    "        if line != '\\n':\n",
    "            token, tag = line.split('\\t')\n",
    "            tags_freq[tag.strip()] += 1\n",
    "            tokens_freq[token.strip()] += 1\n",
    "            current_sentence_tokens.append(token)\n",
    "            current_sentence_tags.append(tag.strip())\n",
    "            if len(current_sentence_tokens)> max_len_sentence:\n",
    "                max_len_sentence = len(current_sentence_tokens)\n",
    "        else:\n",
    "            sentences.append(current_sentence_tokens)\n",
    "            tags.append(current_sentence_tags)\n",
    "            current_sentence_tokens = []\n",
    "            current_sentence_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16275"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'LOCAL': 19326,\n",
       "             'O': 333374,\n",
       "             'ORGANIZACAO': 8747,\n",
       "             'PESSOA': 11274,\n",
       "             'TEMPO': 14079})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _TODO: convert the tag schema to BIO_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_schema(tags, schema='BIO'):\n",
    "    print(tags)\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build a char-level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38530"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_matrix = defaultdict(np.array)\n",
    "char_matrix['PADDING'] = np.random.uniform(-0.5,0.5,25)\n",
    "char_matrix['UNKNOWN'] = np.random.uniform(-0.5,0.5,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for token in tokens_freq:\n",
    "    for char in token:\n",
    "        char_matrix[char] = np.random.uniform(-0.5,0.5,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token2idx = {token: idx for idx, token in enumerate(list(tokens_freq.keys()))}\n",
    "idx2token = {idx: token for token, idx in token2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_token_lenght = max([len(token) for token in token2idx.keys()])\n",
    "\n",
    "all_tokens = []\n",
    "for token in list(token2idx.keys()):\n",
    "    tmp = []\n",
    "    for char in token:\n",
    "        tmp.append(char_matrix[char])\n",
    "    all_tokens.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38530"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38530"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tokens_padded = pad_sequences(all_tokens, maxlen=max_token_lenght, \n",
    "                                  dtype='float32', padding='post', truncating='post', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5362"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx['disse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "word_embeddings = defaultdict(partial(np.ndarray, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an word-embedding layer (_TODO: portuguese embeddings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_fasttext_embeddings():\n",
    "    glove_dir = '/Users/dsbatista/resources/glove.6B'\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def create_embeddings_matrix(embeddings_index, vocabulary, embedding_dim=100):\n",
    "    embeddings_matrix = np.random.rand(len(vocabulary)+1, embedding_dim)\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[i] = embedding_vector\n",
    "    print('Matrix shape: {}'.format(embeddings_matrix.shape))\n",
    "    return embeddings_matrix\n",
    "\n",
    "\n",
    "def get_embeddings_layer(embeddings_matrix, name, max_len, trainable=False):\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=embeddings_matrix.shape[0],\n",
    "        output_dim=embeddings_matrix.shape[1],\n",
    "        input_length=max_len,\n",
    "        weights=[embeddings_matrix],\n",
    "        trainable=trainable,\n",
    "        name=name)\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, concatenate\n",
    "from keras.layers import Input, Dense, Dropout, TimeDistributed\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_idx = load_fasttext_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (38531, 100)\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix = create_embeddings_matrix(embeddings_idx, vocabulary=token2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embedding' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-629c6c947f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_embeddings_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word_embeddings'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-5951f5add902>\u001b[0m in \u001b[0;36mget_embeddings_layer\u001b[0;34m(embeddings_matrix, name, max_len, trainable)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_embeddings_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     embedding_layer = Embedding(\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Embedding' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "word_embeddings_layer = get_embeddings_layer(embeddings_matrix, 'word_embeddings', max_len_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vocab_size = len(token2idx.keys())\n",
    "word_embedding_dim=100\n",
    "embeddings = embeddings_matrix\n",
    "\n",
    "use_char = True\n",
    "char_vocab_size=len(char_matrix.keys())\n",
    "char_embedding_dim=25\n",
    "char_lstm_size=25\n",
    "\n",
    "dropout=0.5\n",
    "word_lstm_size=50\n",
    "\n",
    "use_crf = True\n",
    "\n",
    "fc_dim=100\n",
    "num_labels = len(set([tag for sent_tags in tags for tag in sent_tags]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# biLSTM-CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = Input(batch_shape=(None, None), dtype='int32', name='word_input')\n",
    "inputs = [word_ids]\n",
    "\n",
    "# word embeddings\n",
    "if embeddings is None:\n",
    "    word_embeddings = Embedding(input_dim=word_vocab_size,\n",
    "                                output_dim=word_embedding_dim,\n",
    "                                mask_zero=True,\n",
    "                                name='word_embedding')(word_ids)\n",
    "else:\n",
    "    word_embeddings = Embedding(input_dim=embeddings.shape[0],\n",
    "                                output_dim=embeddings.shape[1],\n",
    "                                mask_zero=True,\n",
    "                                weights=[embeddings],\n",
    "                                name='word_embedding')(word_ids)\n",
    "\n",
    "# build character based word embedding\n",
    "if use_char:\n",
    "    char_ids = Input(batch_shape=(None, None, None), dtype='int32', name='char_input')\n",
    "    inputs.append(char_ids)\n",
    "    char_embeddings = Embedding(input_dim=char_vocab_size,\n",
    "                                output_dim=char_embedding_dim,\n",
    "                                mask_zero=True,\n",
    "                                name='char_embedding')(char_ids)\n",
    "    char_embeddings = TimeDistributed(Bidirectional(LSTM(char_lstm_size)))(char_embeddings)\n",
    "    \n",
    "    print(inputs)\n",
    "    print(word_embeddings.shape)\n",
    "    print(char_embeddings.shape)\n",
    "    \n",
    "    word_embeddings = Concatenate()([word_embeddings, char_embeddings])\n",
    "\n",
    "word_embeddings = Dropout(dropout)(word_embeddings)\n",
    "z = Bidirectional(LSTM(units=word_lstm_size, return_sequences=True))(word_embeddings)\n",
    "z = Dense(fc_dim, activation='tanh')(z)\n",
    "\n",
    "if use_crf:\n",
    "    crf = CRF(num_labels, sparse_target=False)\n",
    "    loss = crf.loss_function\n",
    "    pred = crf(z)\n",
    "else:\n",
    "    loss = 'categorical_crossentropy'\n",
    "    pred = Dense(num_labels, activation='softmax')(z)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag2idx = {}\n",
    "for idx, tag in enumerate(tags_freq.keys()):\n",
    "    tag2idx[tag] = idx\n",
    "idx2tag = {idx : tag for tag, idx in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## build a word-level embeddings and pad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_tokens_embeddings = []\n",
    "tags_encoded = []\n",
    "\n",
    "for sentence, sent_tags in zip(sentences, tags):\n",
    "    sent_embds = []\n",
    "    sent_tags_enc = []\n",
    "    for word, tag in zip(sentence, sent_tags):\n",
    "        if word in embeddings_idx:\n",
    "            embedding = embeddings_idx[word]\n",
    "        else:\n",
    "            embedding = np.random.rand(100)\n",
    "        sent_embds.append(embedding)\n",
    "        sent_tags_enc.append(tag2idx[tag])\n",
    "    sentence_tokens_embeddings.append(sent_embds)\n",
    "    tags_encoded.append(sent_tags_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_tokens_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tags_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padded_sentences_encoded = pad_sequences(sentence_tokens_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(padded_sentences_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded_sentences_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
